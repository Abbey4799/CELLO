model_name,format_score,input_score,task_score,count_limit_score,Average
Llama2-chat-7B,0.598,0.294,0.306,0.686,0.385
Llama2-chat-70B,0.631,0.318,0.265,0.701,0.393
Llama2-chat-13B,0.64,0.342,0.28,0.674,0.402
Vicuna-V1.3-7B,0.598,0.52,0.599,0.597,0.569
WizardLM,0.73,0.525,0.531,0.586,0.574
LongChat-V1-13B,0.723,0.528,0.585,0.507,0.576
LongChat-V1.5-7B,0.791,0.518,0.589,0.535,0.609
LongChat-V1-7B,0.789,0.574,0.615,0.609,0.627
Vicuna-V1.3-13B,0.766,0.588,0.641,0.554,0.631
Vicuna-V1.5-7B,0.756,0.536,0.698,0.599,0.641
Vicuna-V1.3-33B,0.77,0.609,0.668,0.575,0.655
Vicuna-V1.5-13B,0.786,0.656,0.701,0.64,0.699
OpenChat-V3.2,0.766,0.703,0.776,0.617,0.72
GPT-3.5-turbo,0.899,0.76,0.799,0.7,0.794
GPT-4,0.911,0.796,0.792,0.724,0.822
