model_name,extraction,Planning,meta_prompt,brainstorming_single_round,writing_single_round,Ave_complex_ins,keywords_extraction,closed_qa,Summarization,Structure,brainstorming_multi_rounds,writing_multi_rounds,Ave_complex_input,Average
Baize-V2-7b,0.203,0.266,0.3,0.504,0.245,0.304,0.056,0.121,0.045,0.593,0.381,0.558,0.292,0.298
Llama2-FlagAlpha,0.205,0.095,0.129,0.262,0.547,0.248,0.15,0.423,0.297,0.354,0.406,0.591,0.37,0.309
Baize-V2-13b,0.214,0.334,0.342,0.272,0.536,0.34,0.07,0.143,0.019,0.54,0.433,0.574,0.296,0.318
Chinese-Alpaca-V1-13b,0.289,0.183,0.209,0.209,0.697,0.317,0.411,0.272,0.226,0.399,0.291,0.48,0.347,0.332
Chinese-Alpaca-V1-7b,0.264,0.123,0.215,0.357,0.612,0.314,0.265,0.267,0.243,0.465,0.401,0.703,0.391,0.352
Llama2-Linly,0.382,0.17,0.205,0.352,0.527,0.327,0.196,0.464,0.406,0.596,0.352,0.594,0.435,0.381
Chinese-Alpaca-V1-33b,0.379,0.2,0.283,0.664,0.663,0.438,0.415,0.334,0.221,0.426,0.476,0.609,0.413,0.426
BELLE,0.4,0.157,0.363,0.589,0.734,0.449,0.379,0.478,0.508,0.458,0.439,0.672,0.489,0.469
CuteGPT,0.482,0.529,0.46,0.534,0.739,0.549,0.294,0.506,0.459,0.653,0.626,0.804,0.557,0.553
Llama2-LinkSoul,0.521,0.326,0.431,0.652,0.769,0.54,0.615,0.788,0.684,0.565,0.747,0.909,0.718,0.629
Llama2-OpenBuddy,0.585,0.638,0.344,0.697,0.697,0.592,0.638,0.752,0.685,0.711,0.812,0.892,0.748,0.67
